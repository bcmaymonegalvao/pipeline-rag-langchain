{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247665f8-edf7-4a95-8202-66895fd40514",
   "metadata": {
    "id": "247665f8-edf7-4a95-8202-66895fd40514"
   },
   "source": [
    "# **Pipeline RAG com LangChain - embeddings → retriever → LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ff7d83-7d69-4f14-a45c-2cf65ae05785",
   "metadata": {
    "id": "97ff7d83-7d69-4f14-a45c-2cf65ae05785"
   },
   "outputs": [],
   "source": [
    "# Notebook: Pipeline RAG com LangChain - embeddings → retriever → LLM\n",
    "\n",
    "# Objetivo:\n",
    "# Demonstrar um pipeline básico de RAG utilizando LangChain, onde:\n",
    "# 1. Criamos embeddings de um conjunto de documentos textuais.\n",
    "# 2. Construímos um retriever para buscar informações relevantes.\n",
    "# 3. Utilizamos um LLM para gerar respostas contextuais a partir do conteúdo recuperado.\n",
    "\n",
    "# IMPORTANTE:\n",
    "# - Para rodar este notebook você precisa ter:\n",
    "#   * Python 3.8+\n",
    "#   * LangChain instalado (`pip install langchain`)\n",
    "#   * OpenAI SDK instalado (`pip install openai`)\n",
    "#   * Ter uma chave de API OpenAI configurada na variável ambiente OPENAI_API_KEY\n",
    "# - Este é um exemplo simplificado para entendimento do pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0RePhHK3ssEG",
   "metadata": {
    "id": "0RePhHK3ssEG"
   },
   "source": [
    "# **1. IMPORTAÇÕES E INSTALAÇÕES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd40c3d-d587-4dc5-808f-609cb8eecace",
   "metadata": {
    "id": "5dd40c3d-d587-4dc5-808f-609cb8eecace"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instalação de dependências e importação de módulos necessários para o pipeline RAG.\n",
    "\n",
    "1. Instala pacotes via pip:\n",
    "    - faiss-cpu: biblioteca para busca vetorial eficiente (FAISS em CPU).\n",
    "    - sentence-transformers: modelos de embeddings pré-treinados da Hugging Face.\n",
    "    - langchain: núcleo da biblioteca LangChain.\n",
    "    - langchain-community: integrações e conectores mantidos pela comunidade LangChain.\n",
    "\n",
    "2. Importações principais:\n",
    "    - OpenAIEmbeddings (langchain.embeddings):\n",
    "        Geração de embeddings usando a API da OpenAI.\n",
    "    - FAISS (langchain.vectorstores / langchain_community.vectorstores):\n",
    "        Indexação vetorial com FAISS (há duas implementações, a \"core\" e a da comunidade).\n",
    "    - OpenAI (langchain.llms):\n",
    "        Interface para LLMs da OpenAI (ex.: GPT-3.5, GPT-4).\n",
    "    - RetrievalQA (langchain.chains):\n",
    "        Chain que combina retriever + LLM para perguntas e respostas (RAG).\n",
    "    - HuggingFaceEmbeddings (langchain_community.embeddings):\n",
    "        Alternativa open source para gerar embeddings localmente.\n",
    "    - os (módulo padrão do Python):\n",
    "        Usado para ler variáveis de ambiente (como OPENAI_API_KEY).\n",
    "\n",
    "Notas:\n",
    "- O uso de `!pip install` é típico em notebooks (Google Colab, Jupyter).\n",
    "- Recomenda-se fixar versões em ambientes de produção para evitar que mudanças de API quebrem o código.\n",
    "\"\"\"\n",
    "\n",
    "!pip install -q faiss-cpu sentence-transformers langchain langchain-community\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j5LNZPExs0hS",
   "metadata": {
    "id": "j5LNZPExs0hS"
   },
   "source": [
    "# **2. CRIANDO UM CONJUNTO DE DOCUMENTOS DE EXEMPLO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5720d58a-b65b-4ea6-8b8c-31a2aeaca06d",
   "metadata": {
    "id": "5720d58a-b65b-4ea6-8b8c-31a2aeaca06d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a lista de documentos de exemplo a serem indexados no pipeline RAG.\n",
    "\n",
    "Descrição:\n",
    "- Cada item da lista `docs` é uma string representando um conceito central\n",
    "  sobre ciência de dados, NLP e RAG.\n",
    "- Esses documentos simulam uma base de conhecimento reduzida, usada para\n",
    "  demonstração do fluxo embeddings → FAISS → retriever → LLM.\n",
    "\n",
    "Conteúdo:\n",
    "    - Churn: cancelamento ou abandono de clientes.\n",
    "    - NPS (Net Promoter Score): métrica de lealdade de clientes.\n",
    "    - LangChain: biblioteca para integrar LLMs a outras fontes de dados.\n",
    "    - RAG (Retrieval-Augmented Generation): técnica de busca + geração.\n",
    "    - Embeddings: vetores semânticos que representam significado textual.\n",
    "    - Pipeline básico de RAG: embeddings → retriever → LLM.\n",
    "\n",
    "Saída:\n",
    "    docs (list[str]): base textual que será usada para criar embeddings\n",
    "    e treinar o índice FAISS no exemplo.\n",
    "\n",
    "Notas:\n",
    "- Em um caso real, `docs` poderia vir de arquivos externos (PDFs, CSVs, bases SQL).\n",
    "- Quanto maior e mais diversificada a base, mais robusto será o retriever.\n",
    "\"\"\"\n",
    "\n",
    "docs = [\n",
    "    \"O churn é o cancelamento ou abandono de clientes em um serviço ou produto.\",\n",
    "    \"NPS, ou Net Promoter Score, mede a lealdade dos clientes através da pergunta: 'Você recomendaria nossa empresa a um amigo?'\",\n",
    "    \"LangChain é uma biblioteca para construir aplicações que usam modelos de linguagem large (LLMs) integrados a outras fontes de dados.\",\n",
    "    \"RAG, Retrieval-Augmented Generation, conecta modelos de linguagem a bases de conhecimento através de embeddings e mecanismos de busca para melhorar respostas.\",\n",
    "    \"Embeddings representam texto em vetores numéricos que capturam significado semântico, possibilitando busca eficiente por similaridade.\",\n",
    "    \"O pipeline básico de RAG inclui: criação de embeddings, uso do retriever para buscar documentos relevantes e geração da resposta pelo LLM.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LKrQHWtqtaIP",
   "metadata": {
    "id": "LKrQHWtqtaIP"
   },
   "source": [
    "# **3. CONSTRUÇÃO DOS EMBEDDINGS DOS DOCUMENTOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a6f3c5-8fd1-4ec5-9298-6a4e171c2522",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91a6f3c5-8fd1-4ec5-9298-6a4e171c2522",
    "outputId": "ce59ff78-b5b9-4c8d-bf41-ba0567d6e480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando embeddings dos documentos com OpenAIEmbeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2911721343.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos indexados: 6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Criação dos embeddings e construção do índice vetorial FAISS.\n",
    "\n",
    "Etapas:\n",
    "1. Inicializa um modelo de embeddings semânticos local usando Hugging Face.\n",
    "   - Modelo: \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "   - Esse modelo gera vetores densos de 384 dimensões, balanceando qualidade e velocidade.\n",
    "   - Alternativa: usar OpenAIEmbeddings, se houver chave API configurada.\n",
    "\n",
    "2. Constrói um índice FAISS a partir da lista de documentos (`docs`).\n",
    "   - `FAISS.from_texts(docs, embeddings)` transforma cada documento em vetor\n",
    "     e armazena em uma estrutura de busca vetorial eficiente.\n",
    "   - Esse índice permite consultas de similaridade para recuperação posterior.\n",
    "\n",
    "Args:\n",
    "    docs (list[str]): lista de documentos em texto plano que serão indexados.\n",
    "\n",
    "Saídas:\n",
    "    embeddings (HuggingFaceEmbeddings): modelo de geração de embeddings semânticos.\n",
    "    vectorstore (FAISS): índice FAISS com embeddings prontos para busca de similaridade.\n",
    "\n",
    "Notas:\n",
    "- Para datasets maiores, considere salvar o índice com `vectorstore.save_local(path)`\n",
    "  e recarregar depois com `FAISS.load_local(path, embeddings)`.\n",
    "- O modelo MiniLM é leve, ideal para protótipos e uso em CPU; versões maiores podem\n",
    "  trazer mais qualidade, mas exigem GPU.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Criando embeddings dos documentos com OpenAIEmbeddings...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Criar vetor store FAISS para indexar e buscar similaridades\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "print(f\"Documentos indexados: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WwPEyMs3txE8",
   "metadata": {
    "id": "WwPEyMs3txE8"
   },
   "source": [
    "# **4. CRIAR O RETRIEVER A PARTIR DO VECTORSTORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dec1615-b03f-4b35-a23f-fcfc2f10cf1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dec1615-b03f-4b35-a23f-fcfc2f10cf1c",
    "outputId": "95ce29a9-d9bf-4439-e636-783bdb65f618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever pronto (k=2).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cria o retriever a partir do índice vetorial FAISS.\n",
    "\n",
    "Um retriever é responsável por recuperar os documentos mais relevantes\n",
    "a partir de uma consulta, com base na similaridade dos embeddings.\n",
    "\n",
    "Configuração usada:\n",
    "    search_type (str): \"similarity\"\n",
    "        - Calcula a proximidade vetorial entre a query e os documentos.\n",
    "        - Outros tipos possíveis em LangChain: \"mmr\" (Maximal Marginal Relevance).\n",
    "    search_kwargs (dict): {\"k\": 2}\n",
    "        - Número de documentos mais semelhantes a serem retornados (top-k).\n",
    "        - Ajustar `k` pode equilibrar entre precisão (baixo k) e cobertura/contexto (alto k).\n",
    "\n",
    "Saída:\n",
    "    retriever (BaseRetriever): objeto compatível com LangChain que pode ser\n",
    "    passado para `RetrievalQA` ou outras chains.\n",
    "\n",
    "Notas:\n",
    "- Este retriever é a ponte entre embeddings (docs) e o LLM.\n",
    "- Se `k` for muito baixo, pode perder contexto; se for muito alto, pode adicionar ruído.\n",
    "\"\"\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "print(\"Retriever pronto (k=2).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Of07gnT4t7_m",
   "metadata": {
    "id": "Of07gnT4t7_m"
   },
   "source": [
    "# **5. CONFIGURAR O MODELO LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fe0bb6-1d1e-47f7-b5db-b8ddb87174fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37fe0bb6-1d1e-47f7-b5db-b8ddb87174fe",
    "outputId": "81bcf42a-e3dd-4c0e-9cc0-981f344a2b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando o LLM local (Hugging Face - FLAN-T5 base)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipython-input-425815035.py:13: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=gen_pipe)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuração do LLM (Language Model) para o pipeline RAG.\n",
    "\n",
    "Este bloco implementa um fallback local usando Hugging Face caso a chave da OpenAI\n",
    "não esteja configurada no ambiente. O modelo escolhido é o `google/flan-t5-base`,\n",
    "um modelo seq2seq leve, adequado para CPU ou GPU no Google Colab.\n",
    "\n",
    "Passos:\n",
    "1. Verifica se a variável de ambiente `OPENAI_API_KEY` está definida.\n",
    "   - Se sim, o notebook poderia configurar um LLM da OpenAI (não implementado aqui).\n",
    "   - Se não, usa o FLAN-T5 como LLM local.\n",
    "2. Carrega o tokenizer e o modelo pré-treinado da Hugging Face.\n",
    "3. Cria um `pipeline` de geração de texto (`text2text-generation`) limitado a 256 tokens.\n",
    "4. Encapsula o pipeline no wrapper `HuggingFacePipeline` do LangChain, para integração uniforme.\n",
    "\n",
    "Args principais:\n",
    "    model_name (str): nome do modelo Hugging Face a ser carregado.\n",
    "    max_new_tokens (int): limite de tokens gerados por resposta (ajustável conforme GPU/CPU).\n",
    "\n",
    "Saída:\n",
    "    llm (HuggingFacePipeline): instância de LLM integrada ao LangChain, pronta para uso no RetrievalQA.\n",
    "\n",
    "Notas:\n",
    "- Para respostas mais longas, aumente `max_new_tokens`.\n",
    "- Em máquinas com GPU, pode-se mover o modelo para CUDA:\n",
    "      mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
    "      gen_pipe = pipeline(..., device=0)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "USE_OPENAI = \"OPENAI_API_KEY\" in os.environ and len(os.environ[\"OPENAI_API_KEY\"].strip()) > 0\n",
    "\n",
    "print(\"Configurando o LLM local (Hugging Face - FLAN-T5 base)...\")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "model_name = \"google/flan-t5-base\"  # leve; bom p/ CPU/GPU do Colab\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "gen_pipe = pipeline(\"text2text-generation\", model=mdl, tokenizer=tok, max_new_tokens=256)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cz6HvAMbxQo2",
   "metadata": {
    "id": "Cz6HvAMbxQo2"
   },
   "source": [
    "# **6. CRIAR A CHAIN DE RAG: COMBINA RETRIEVER E LLM PARA RESPOSTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40e91b6-5724-44e6-9d30-b8c97a55fe49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f40e91b6-5724-44e6-9d30-b8c97a55fe49",
    "outputId": "fc19bc1b-0570-436c-9b67-37f0919eb7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain pronta.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cria a chain de RAG (Retrieval-Augmented Generation) usando LangChain.\n",
    "\n",
    "A classe `RetrievalQA` combina:\n",
    "- Um retriever (responsável por buscar documentos relevantes no índice vetorial).\n",
    "- Um LLM (responsável por gerar a resposta contextualizada).\n",
    "\n",
    "Args configurados:\n",
    "    llm (BaseLanguageModel): Modelo de linguagem já inicializado\n",
    "        (ex.: OpenAI, HuggingFacePipeline).\n",
    "    retriever (BaseRetriever): Retriever baseado em FAISS configurado na célula anterior.\n",
    "    return_source_documents (bool): \n",
    "        - False → retorna apenas a resposta gerada.\n",
    "        - True  → retorna também os documentos de origem usados na resposta.\n",
    "\n",
    "Saída:\n",
    "    qa_chain (RetrievalQA): instância pronta para receber queries no formato\n",
    "    `qa_chain.invoke({\"query\": \"sua pergunta\"})`.\n",
    "\n",
    "Notas:\n",
    "- Este é o núcleo do pipeline RAG: conecta embeddings → retriever → LLM.\n",
    "- Se `return_source_documents=True`, pode ser útil para depuração e explicabilidade.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,             # usa o FAISS retriever criado antes\n",
    "    return_source_documents=False    # defina True se quiser retornar também os docs relevantes\n",
    ")\n",
    "print(\"RAG chain pronta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upsTEfyTw80X",
   "metadata": {
    "id": "upsTEfyTw80X"
   },
   "source": [
    "# **7. REALIZAR PERGUNTAS E GERAR RESPOSTAS USANDO O PIPELINE RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05966c7d-796c-418a-b2a5-3372cf17182e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05966c7d-796c-418a-b2a5-3372cf17182e",
    "outputId": "fde45be7-fd20-4e16-bf57-32ab70772932"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-602582006.py:11: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resposta = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perguntas e respostas no pipeline RAG:\n",
      "\n",
      "Pergunta: O que significa churn em análise de clientes?\n",
      "Resposta: NPS, or Net Promoter Score, mede a lealdade dos clientes através da pergunta: 'Você recomendaria nossa empresa a um amigo?'\n",
      "------------------------------------------------------------\n",
      "Pergunta: Como funciona o pipeline de RAG?\n",
      "Resposta: criaço de embeddings, uso do retriever para buscar documents relevantes e geraço da resposta pelo LLM\n",
      "------------------------------------------------------------\n",
      "Pergunta: Explique o que é LangChain.\n",
      "Resposta: a biblioteca para construir aplicaçes que usam modelos de linguagem large (LLMs) integrados a outras fontes de dados\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Executa um ciclo de Perguntas & Respostas usando a chain de RAG.\n",
    "\n",
    "Esta célula:\n",
    "- Define perguntas de teste (FAQ rápida).\n",
    "- Invoca a chain `qa_chain` com o método moderno `invoke` (substitui `run`, que está deprecado).\n",
    "- Imprime o resultado de cada pergunta com um separador.\n",
    "\n",
    "Pré-requisitos:\n",
    "- `qa_chain`: instância de `RetrievalQA` já configurada com `llm` e `retriever`.\n",
    "\n",
    "Notas:\n",
    "- `RetrievalQA.invoke` aceita um dicionário com a chave \"query\".\n",
    "- Dependendo da versão do LangChain/chain, o retorno pode ser:\n",
    "  - um `dict` com a chave \"result\" (mais comum), ou\n",
    "  - uma string direta (fallback tratado abaixo).\n",
    "\"\"\"\n",
    "\n",
    "query_1 = \"O que significa churn em análise de clientes?\"\n",
    "query_2 = \"Como funciona o pipeline de RAG?\"\n",
    "query_3 = \"Explique o que é LangChain.\"\n",
    "\n",
    "print(\"\\nPerguntas e respostas no pipeline RAG:\\n\")\n",
    "\n",
    "for query in [query_1, query_2, query_3]:\n",
    "    print(f\"Pergunta: {query}\")\n",
    "    # Uso moderno do LangChain (evita DeprecationWarning do .run)\n",
    "    out = qa_chain.invoke({\"query\": query})\n",
    "    resposta = out.get(\"result\") if isinstance(out, dict) else out\n",
    "    print(f\"Resposta: {resposta}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kCc-Ap2qww-i",
   "metadata": {
    "id": "kCc-Ap2qww-i"
   },
   "source": [
    "# **8. CONCLUSÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61955268-af25-4cfc-a0e4-5c97cf9dbbd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61955268-af25-4cfc-a0e4-5c97cf9dbbd2",
    "outputId": "6c8624dd-31c8-4e17-8c59-99c5578048b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neste notebook você viu como:\n",
      "- Criar embeddings semânticos para documentos usando OpenAIEmbeddings.\n",
      "- Indexar documentos em um vetor store FAISS para busca eficiente.\n",
      "- Usar o retriever para buscar os documentos mais relevantes para uma query.\n",
      "- Passar o conteúdo recuperado para um LLM gerar respostas contextuais e contextualizadas.\n",
      "Este é o fluxo típico da pipeline RAG com LangChain: embeddings → retriever → LLM.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imprime um resumo do que foi feito no notebook.\n",
    "\n",
    "Pontos-chave:\n",
    "- Geração de embeddings semânticos com HuggingFaceEmbeddings (ou OpenAIEmbeddings, se configurado).\n",
    "- Indexação FAISS para busca vetorial eficiente.\n",
    "- Uso de um retriever (similaridade) para selecionar contexto relevante.\n",
    "- Combinação com um LLM (OpenAI ou local via Transformers) para respostas fundamentadas (RAG).\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Neste notebook você viu como:\n",
    "- Criar embeddings semânticos para documentos usando HuggingFaceEmbeddings (ou OpenAIEmbeddings, se preferir).\n",
    "- Indexar documentos em um vetor store FAISS para busca eficiente.\n",
    "- Usar o retriever para buscar os documentos mais relevantes para uma query.\n",
    "- Passar o conteúdo recuperado para um LLM gerar respostas contextuais.\n",
    "Este é o fluxo típico da pipeline RAG com LangChain: embeddings → retriever → LLM.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "QwUHGARMwFy9",
   "metadata": {
    "id": "QwUHGARMwFy9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
