{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247665f8-edf7-4a95-8202-66895fd40514",
   "metadata": {
    "id": "247665f8-edf7-4a95-8202-66895fd40514"
   },
   "source": [
    "# **Pipeline RAG com LangChain - embeddings → retriever → LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ff7d83-7d69-4f14-a45c-2cf65ae05785",
   "metadata": {
    "id": "97ff7d83-7d69-4f14-a45c-2cf65ae05785"
   },
   "outputs": [],
   "source": [
    "# Notebook: Pipeline RAG com LangChain - embeddings → retriever → LLM\n",
    "\n",
    "# Objetivo:\n",
    "# Demonstrar um pipeline básico de RAG utilizando LangChain, onde:\n",
    "# 1. Criamos embeddings de um conjunto de documentos textuais.\n",
    "# 2. Construímos um retriever para buscar informações relevantes.\n",
    "# 3. Utilizamos um LLM para gerar respostas contextuais a partir do conteúdo recuperado.\n",
    "\n",
    "# IMPORTANTE:\n",
    "# - Para rodar este notebook você precisa ter:\n",
    "#   * Python 3.8+\n",
    "#   * LangChain instalado (`pip install langchain`)\n",
    "#   * OpenAI SDK instalado (`pip install openai`)\n",
    "#   * Ter uma chave de API OpenAI configurada na variável ambiente OPENAI_API_KEY\n",
    "# - Este é um exemplo simplificado para entendimento do pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0RePhHK3ssEG",
   "metadata": {
    "id": "0RePhHK3ssEG"
   },
   "source": [
    "# **1. IMPORTAÇÕES E INSTALAÇÕES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd40c3d-d587-4dc5-808f-609cb8eecace",
   "metadata": {
    "id": "5dd40c3d-d587-4dc5-808f-609cb8eecace"
   },
   "outputs": [],
   "source": [
    "!pip install -q faiss-cpu sentence-transformers langchain langchain-community\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j5LNZPExs0hS",
   "metadata": {
    "id": "j5LNZPExs0hS"
   },
   "source": [
    "# **2. CRIANDO UM CONJUNTO DE DOCUMENTOS DE EXEMPLO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5720d58a-b65b-4ea6-8b8c-31a2aeaca06d",
   "metadata": {
    "id": "5720d58a-b65b-4ea6-8b8c-31a2aeaca06d"
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"O churn é o cancelamento ou abandono de clientes em um serviço ou produto.\",\n",
    "    \"NPS, ou Net Promoter Score, mede a lealdade dos clientes através da pergunta: 'Você recomendaria nossa empresa a um amigo?'\",\n",
    "    \"LangChain é uma biblioteca para construir aplicações que usam modelos de linguagem large (LLMs) integrados a outras fontes de dados.\",\n",
    "    \"RAG, Retrieval-Augmented Generation, conecta modelos de linguagem a bases de conhecimento através de embeddings e mecanismos de busca para melhorar respostas.\",\n",
    "    \"Embeddings representam texto em vetores numéricos que capturam significado semântico, possibilitando busca eficiente por similaridade.\",\n",
    "    \"O pipeline básico de RAG inclui: criação de embeddings, uso do retriever para buscar documentos relevantes e geração da resposta pelo LLM.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LKrQHWtqtaIP",
   "metadata": {
    "id": "LKrQHWtqtaIP"
   },
   "source": [
    "# **3. CONSTRUÇÃO DOS EMBEDDINGS DOS DOCUMENTOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a6f3c5-8fd1-4ec5-9298-6a4e171c2522",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91a6f3c5-8fd1-4ec5-9298-6a4e171c2522",
    "outputId": "ce59ff78-b5b9-4c8d-bf41-ba0567d6e480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando embeddings dos documentos com OpenAIEmbeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2911721343.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos indexados: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Criando embeddings dos documentos com OpenAIEmbeddings...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Criar vetor store FAISS para indexar e buscar similaridades\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "print(f\"Documentos indexados: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WwPEyMs3txE8",
   "metadata": {
    "id": "WwPEyMs3txE8"
   },
   "source": [
    "# **4. CRIAR O RETRIEVER A PARTIR DO VECTORSTORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dec1615-b03f-4b35-a23f-fcfc2f10cf1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dec1615-b03f-4b35-a23f-fcfc2f10cf1c",
    "outputId": "95ce29a9-d9bf-4439-e636-783bdb65f618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever pronto (k=2).\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "print(\"Retriever pronto (k=2).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Of07gnT4t7_m",
   "metadata": {
    "id": "Of07gnT4t7_m"
   },
   "source": [
    "# **5. CONFIGURAR O MODELO LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fe0bb6-1d1e-47f7-b5db-b8ddb87174fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37fe0bb6-1d1e-47f7-b5db-b8ddb87174fe",
    "outputId": "81bcf42a-e3dd-4c0e-9cc0-981f344a2b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando o LLM local (Hugging Face - FLAN-T5 base)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipython-input-425815035.py:13: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=gen_pipe)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "USE_OPENAI = \"OPENAI_API_KEY\" in os.environ and len(os.environ[\"OPENAI_API_KEY\"].strip()) > 0\n",
    "\n",
    "print(\"Configurando o LLM local (Hugging Face - FLAN-T5 base)...\")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "model_name = \"google/flan-t5-base\"  # leve; bom p/ CPU/GPU do Colab\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "gen_pipe = pipeline(\"text2text-generation\", model=mdl, tokenizer=tok, max_new_tokens=256)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cz6HvAMbxQo2",
   "metadata": {
    "id": "Cz6HvAMbxQo2"
   },
   "source": [
    "# **6. CRIAR A CHAIN DE RAG: COMBINA RETRIEVER E LLM PARA RESPOSTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40e91b6-5724-44e6-9d30-b8c97a55fe49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f40e91b6-5724-44e6-9d30-b8c97a55fe49",
    "outputId": "fc19bc1b-0570-436c-9b67-37f0919eb7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain pronta.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,             # usa o FAISS retriever\n",
    "    return_source_documents=False    # mude para True se quiser ver as fontes\n",
    ")\n",
    "print(\"RAG chain pronta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upsTEfyTw80X",
   "metadata": {
    "id": "upsTEfyTw80X"
   },
   "source": [
    "# **7. REALIZAR PERGUNTAS E GERAR RESPOSTAS USANDO O PIPELINE RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05966c7d-796c-418a-b2a5-3372cf17182e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05966c7d-796c-418a-b2a5-3372cf17182e",
    "outputId": "fde45be7-fd20-4e16-bf57-32ab70772932"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-602582006.py:11: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resposta = qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perguntas e respostas no pipeline RAG:\n",
      "\n",
      "Pergunta: O que significa churn em análise de clientes?\n",
      "Resposta: NPS, or Net Promoter Score, mede a lealdade dos clientes através da pergunta: 'Você recomendaria nossa empresa a um amigo?'\n",
      "------------------------------------------------------------\n",
      "Pergunta: Como funciona o pipeline de RAG?\n",
      "Resposta: criaço de embeddings, uso do retriever para buscar documents relevantes e geraço da resposta pelo LLM\n",
      "------------------------------------------------------------\n",
      "Pergunta: Explique o que é LangChain.\n",
      "Resposta: a biblioteca para construir aplicaçes que usam modelos de linguagem large (LLMs) integrados a outras fontes de dados\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"O que significa churn em análise de clientes?\"\n",
    "query_2 = \"Como funciona o pipeline de RAG?\"\n",
    "query_3 = \"Explique o que é LangChain.\"\n",
    "\n",
    "print(\"\\nPerguntas e respostas no pipeline RAG:\\n\")\n",
    "for query in [query_1, query_2, query_3]:\n",
    "    print(f\"Pergunta: {query}\")\n",
    "    resposta = qa_chain.run(query)\n",
    "    print(f\"Resposta: {resposta}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kCc-Ap2qww-i",
   "metadata": {
    "id": "kCc-Ap2qww-i"
   },
   "source": [
    "# **8. CONCLUSÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61955268-af25-4cfc-a0e4-5c97cf9dbbd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61955268-af25-4cfc-a0e4-5c97cf9dbbd2",
    "outputId": "6c8624dd-31c8-4e17-8c59-99c5578048b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neste notebook você viu como:\n",
      "- Criar embeddings semânticos para documentos usando OpenAIEmbeddings.\n",
      "- Indexar documentos em um vetor store FAISS para busca eficiente.\n",
      "- Usar o retriever para buscar os documentos mais relevantes para uma query.\n",
      "- Passar o conteúdo recuperado para um LLM gerar respostas contextuais e contextualizadas.\n",
      "Este é o fluxo típico da pipeline RAG com LangChain: embeddings → retriever → LLM.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Neste notebook você viu como:\n",
    "- Criar embeddings semânticos para documentos usando OpenAIEmbeddings.\n",
    "- Indexar documentos em um vetor store FAISS para busca eficiente.\n",
    "- Usar o retriever para buscar os documentos mais relevantes para uma query.\n",
    "- Passar o conteúdo recuperado para um LLM gerar respostas contextuais e contextualizadas.\n",
    "Este é o fluxo típico da pipeline RAG com LangChain: embeddings → retriever → LLM.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "QwUHGARMwFy9",
   "metadata": {
    "id": "QwUHGARMwFy9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
