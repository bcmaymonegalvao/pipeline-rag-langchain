"""
MIGUEL ‚Äî Modelo Interativo Generativo de Linguagem para Uso Educacional Livre
----------------------------------------------------------------------------

Aplicativo did√°tico (Streamlit) que demonstra um pipeline RAG (Retrieval-Augmented Generation)
com componentes 100% locais e gratuitos:
- Representa√ß√µes vetoriais (MiniLM) + FAISS (busca por similaridade)
- LLM (FLAN-T5) para gera√ß√£o de resposta
- Upload de PDFs para ampliar a base de conhecimento

Objetivo: permitir que estudantes e professores aprendam IA ‚Äúpor dentro‚Äù, personalizando e
observando as etapas do pipeline, com uma interface clara e documenta√ß√£o acess√≠vel.

Notas:
- Tema: paleta clara (principal) com acentos escuros (auxiliares).
- Evita TypeError ao formatar m√©tricas quando o hist√≥rico est√° vazio.
"""

from __future__ import annotations

import logging
import os
import pickle
import tempfile
import time
from typing import Any, Dict, List, Optional, Tuple

import psutil
import streamlit as st


# =============================================================================
# Configura√ß√£o geral do app
# =============================================================================

APP_NAME = "Modelo Interativo Generativo de Linguagem para Uso Educacional Livre"
APP_SHORT = "MIGUEL"
APP_SUBTITLE = "Aplicativo did√°tico (RAG) com LLM local, representa√ß√µes vetoriais e FAISS ‚Äî sem API keys"

st.set_page_config(
    page_title=f"{APP_SHORT} ‚Äî {APP_NAME}",
    page_icon="üß©",
    layout="wide",
    initial_sidebar_state="expanded",
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# Estilo (CSS) + utilidades de UI
# =============================================================================

def inject_minimal_css() -> None:
    """Tema claro (principal) com acentos escuros (auxiliares) e alto contraste no t√≠tulo."""
    st.markdown(
        """
        <style>
          :root{
            /* Base clara */
            --bg: #f7f9fc;
            --panel: #ffffff;
            --card: #ffffff;

            /* Texto */
            --text: #0b1220;
            --muted: #475569;

            /* Linhas/bordas */
            --line: #d6deea;

            /* Acento (auxiliar escuro) */
            --accent: #1e3a8a;       /* azul escuro */
            --accentSoft: #e8f0ff;   /* azul bem claro */

            /* Estados */
            --ok: #15803d;
            --warn: #b45309;
            --err: #b91c1c;
          }

          /* Fundo geral */
          .stApp {
            background: var(--bg);
            color: var(--text);
          }

          /* Sidebar clara */
          section[data-testid="stSidebar"]{
            background: var(--panel);
            border-right: 1px solid var(--line);
          }

          /* T√≠tulo com contraste alto */
          .miguel-title {
            font-size: 2.2rem;
            font-weight: 900;
            letter-spacing: -0.02em;
            text-align: left;
            margin: 0.25rem 0 0.25rem 0;
            color: var(--text);
          }

          .miguel-subtitle {
            color: var(--muted);
            font-size: 1.05rem;
            margin: 0 0 1.15rem 0;
          }

          /* Cards */
          .miguel-card {
            background: var(--card);
            border: 1px solid var(--line);
            border-radius: 14px;
            padding: 1rem 1.1rem;
            margin: 0.6rem 0;
            box-shadow: 0 1px 0 rgba(2, 6, 23, 0.03);
          }

          .miguel-card h3, .miguel-card h4 {
            margin: 0 0 0.35rem 0;
            color: var(--text);
          }

          .miguel-card p {
            margin: 0.2rem 0 0 0;
            color: var(--muted);
          }

          /* Pills */
          .miguel-pill {
            display: inline-block;
            padding: 0.15rem 0.55rem;
            border-radius: 999px;
            border: 1px solid var(--line);
            background: var(--accentSoft);
            color: var(--accent);
            font-weight: 600;
            font-size: 0.85rem;
            margin-right: 0.35rem;
          }

          /* Status */
          .state-ok    { color: var(--ok); }
          .state-warn  { color: var(--warn); }
          .state-err   { color: var(--err); }

          /* Links */
          a { color: var(--accent); }

          /* Bot√µes */
          .stButton > button {
            border-radius: 10px;
            border: 1px solid var(--line);
          }

          /* Espa√ßamento superior */
          .block-container { padding-top: 1.0rem; }
        </style>
        """,
        unsafe_allow_html=True,
        /* --- Evitar que o header do Streamlit cubra o conte√∫do --- */
        header[data-testid="stHeader"]{
          background: transparent;   /* n√£o pinta por cima do t√≠tulo */
        }
        
        /* Empurra a √°rea principal para baixo, evitando corte do t√≠tulo */
        div[data-testid="stAppViewContainer"] > .main {
          padding-top: 4.25rem;
        }
        
        /* Mant√©m o espa√ßamento interno do container (conte√∫do) */
        .block-container {
          padding-top: 0.75rem;
        }
        
        /* Em telas menores, o header pode ser maior */
        @media (max-width: 768px){
        div[data-testid="stAppViewContainer"] > .main {
            padding-top: 5.0rem;
          }
        }
    )


def render_header() -> None:
    """Renderiza cabe√ßalho do app (sem emoji de foguete)."""
    st.markdown(f"<div class='miguel-title'>{APP_NAME}</div>", unsafe_allow_html=True)
    st.markdown(f"<div class='miguel-subtitle'>{APP_SUBTITLE}</div>", unsafe_allow_html=True)
    st.markdown(
        "<hr style='border:none;border-top:1px solid #d6deea;margin:0.6rem 0 1.0rem 0;'>",
        unsafe_allow_html=True,
    )


def card(title: str, body_md: str, pills: Optional[List[str]] = None) -> None:
    """
    Renderiza um card para agrupar conte√∫do (Gestalt: regi√£o comum + proximidade).

    Args:
        title: T√≠tulo do card.
        body_md: Conte√∫do em Markdown/HTML simples.
        pills: Pequenos r√≥tulos para refor√ßar a leitura (similaridade).
    """
    pills_html = ""
    if pills:
        pills_html = "".join([f"<span class='miguel-pill'>{p}</span>" for p in pills])

    st.markdown(
        f"""
        <div class="miguel-card">
          <h3>{title}</h3>
          {pills_html}
          <div style="margin-top:0.45rem">{body_md}</div>
        </div>
        """,
        unsafe_allow_html=True,
    )


# =============================================================================
# Estado da sess√£o e persist√™ncia local
# =============================================================================

DATA_DIR = "data"
CUSTOM_DOCS_PATH = os.path.join(DATA_DIR, "custom_docs.pkl")


def init_session_state() -> None:
    """Inicializa vari√°veis em st.session_state para consist√™ncia e preven√ß√£o de erros."""
    st.session_state.setdefault("page", "Chat")
    st.session_state.setdefault("query_history", [])
    st.session_state.setdefault("docs", [])
    st.session_state.setdefault("qa_chain", None)
    st.session_state.setdefault("vectorstore", None)
    st.session_state.setdefault("retriever", None)
    st.session_state.setdefault("embeddings", None)

    # Par√¢metros interativos
    st.session_state.setdefault("retriever_k", 3)
    st.session_state.setdefault("max_new_tokens", 512)
    st.session_state.setdefault("temperature", 0.7)

    st.session_state.setdefault("toast", None)


def save_custom_docs(docs_list: List[str]) -> bool:
    """Salva a lista de documentos customizados localmente em arquivo pickle."""
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(CUSTOM_DOCS_PATH, "wb") as f:
            pickle.dump(docs_list, f)
        return True
    except Exception as e:
        logger.error(f"Erro ao salvar documentos: {e}")
        return False


def load_custom_docs() -> Optional[List[str]]:
    """Carrega documentos customizados salvos, se existirem."""
    try:
        if os.path.exists(CUSTOM_DOCS_PATH):
            with open(CUSTOM_DOCS_PATH, "rb") as f:
                return pickle.load(f)
        return None
    except Exception as e:
        logger.error(f"Erro ao carregar documentos: {e}")
        return None


# =============================================================================
# Conte√∫do did√°tico (documentos padr√£o + gloss√°rio)
# =============================================================================

def get_default_docs() -> List[str]:
    """Retorna uma base m√≠nima de documentos de exemplo para o pipeline."""
    return [
        "Churn √© o cancelamento/abandono de clientes em um servi√ßo. √â uma m√©trica importante de reten√ß√£o.",
        "NPS (Net Promoter Score) mede lealdade perguntando se o cliente recomendaria a empresa; varia de -100 a +100.",
        "LangChain √© um framework para construir aplica√ß√µes com modelos de linguagem e componentes (mem√≥ria, ferramentas, dados).",
        "RAG (Retrieval-Augmented Generation) conecta um modelo de linguagem a uma base de conhecimento para responder com evid√™ncias.",
        "Representa√ß√µes vetoriais (embeddings) transformam texto em n√∫meros, preservando o significado para buscas por similaridade.",
        "Um pipeline RAG t√≠pico: dividir texto (chunking), gerar embeddings, indexar, recuperar trechos relevantes e gerar resposta.",
        "Machine Learning √© uma √°rea da IA em que sistemas aprendem padr√µes a partir de dados para tomar decis√µes ou fazer previs√µes.",
        "Deep Learning usa redes neurais com muitas camadas para lidar com padr√µes complexos (texto, imagem, √°udio).",
        "NLP (Processamento de Linguagem Natural) √© a √°rea da IA que lida com compreens√£o e gera√ß√£o de linguagem humana.",
        "BI (Business Intelligence) re√∫ne pr√°ticas e ferramentas para an√°lise de dados e suporte √† decis√£o em neg√≥cios.",
    ]


GLOSSARY: Dict[str, Dict[str, str]] = {
    "LLM (Large Language Model)": {
        "o_que_e": "Um modelo de linguagem em grande escala: aprende padr√µes de texto e consegue gerar respostas em linguagem natural.",
        "onde_aparece_no_app": "√â o componente que gera a resposta final (aqui: FLAN-T5).",
        "por_que_importa": "Define a qualidade do texto gerado, mas pode ‚Äòalucinar‚Äô ‚Äî por isso usamos RAG com documentos.",
    },
    "RAG (Gera√ß√£o aumentada por recupera√ß√£o)": {
        "o_que_e": "Arquitetura que busca trechos relevantes em documentos e inclui esse contexto antes do modelo gerar a resposta.",
        "onde_aparece_no_app": "No ‚Äòretriever‚Äô + ‚Äòvector store‚Äô, que selecionam os trechos e os entregam ao modelo.",
        "por_que_importa": "Aumenta a chance de resposta correta e ancorada em evid√™ncias (reduz alucina√ß√µes).",
    },
    "Chunking (Divis√£o em trechos)": {
        "o_que_e": "Processo de dividir textos longos em partes menores para indexa√ß√£o e busca.",
        "onde_aparece_no_app": "Ao processar PDFs: o texto √© cortado em trechos com sobreposi√ß√£o.",
        "por_que_importa": "Trechos menores tornam a busca por similaridade mais eficiente e o contexto mais √∫til.",
    },
    "Representa√ß√µes vetoriais (Embeddings)": {
        "o_que_e": "Transforma√ß√£o do texto em um vetor num√©rico que ‚Äúrepresenta o significado‚Äù.",
        "onde_aparece_no_app": "Usadas para indexar documentos e comparar similaridade com a pergunta.",
        "por_que_importa": "Permite busca sem√¢ntica (por sentido), n√£o apenas por palavras exatas.",
    },
    "FAISS (Vector Store)": {
        "o_que_e": "Biblioteca de busca eficiente por similaridade entre vetores.",
        "onde_aparece_no_app": "Armazena embeddings e retorna os trechos mais pr√≥ximos da pergunta.",
        "por_que_importa": "Acelera a recupera√ß√£o de informa√ß√µes mesmo com milhares de trechos.",
    },
    "Retriever (Recuperador)": {
        "o_que_e": "Componente que consulta o √≠ndice vetorial e retorna os Top-k trechos mais relevantes.",
        "onde_aparece_no_app": "Configura√ß√£o ‚Äòk‚Äô (Top-k) influencia quantos trechos s√£o enviados ao modelo.",
        "por_que_importa": "Poucos trechos ‚Üí pode faltar contexto; muitos trechos ‚Üí pode confundir o modelo.",
    },
    "Top-k (k)": {
        "o_que_e": "Quantidade de trechos retornados pela busca por similaridade.",
        "onde_aparece_no_app": "Configura√ß√£o na barra lateral (Configura√ß√µes do pipeline).",
        "por_que_importa": "Equilibra contexto suficiente e ru√≠do excessivo.",
    },
    "Temperatura (temperature)": {
        "o_que_e": "Controla aleatoriedade do texto gerado: menor = mais previs√≠vel; maior = mais criativo.",
        "onde_aparece_no_app": "Configura√ß√£o na barra lateral (Configura√ß√µes do pipeline).",
        "por_que_importa": "Em contexto did√°tico, valores menores tendem a gerar respostas mais consistentes.",
    },
    "max_new_tokens": {
        "o_que_e": "Limite m√°ximo do tamanho da resposta (em tokens).",
        "onde_aparece_no_app": "Configura√ß√£o na barra lateral (Configura√ß√µes do pipeline).",
        "por_que_importa": "Controla tempo/uso de recursos e evita respostas longas demais.",
    },
}


# =============================================================================
# M√©tricas e seguran√ßa (recursos locais)
# =============================================================================

def get_system_stats() -> Dict[str, float]:
    """Coleta estat√≠sticas b√°sicas do sistema e do √≠ndice vetorial."""
    ram = psutil.virtual_memory()

    faiss_size_mb = 0.0
    total_vectors = 0
    total_docs = len(st.session_state.docs) if st.session_state.get("docs") else 0

    if st.session_state.get("vectorstore") is not None:
        try:
            faiss_index = st.session_state.vectorstore.index
            total_vectors = int(faiss_index.ntotal)
            faiss_size_mb = (total_vectors * 4) / (1024 * 1024)
        except Exception as e:
            logger.warning(f"Erro ao obter dados do FAISS: {e}")

    return {
        "ram_used_gb": ram.used / (1024 ** 3),
        "ram_total_gb": ram.total / (1024 ** 3),
        "faiss_size_mb": float(faiss_size_mb),
        "total_vectors": float(total_vectors),
        "total_docs": float(total_docs),
    }


def check_system_safety() -> Dict[str, Any]:
    """Verifica limites ‚Äúseguros‚Äù de uso para evitar travamentos por excesso de RAM/√≠ndice."""
    stats = get_system_stats()
    ram_usage_pct = stats["ram_used_gb"] / stats["ram_total_gb"] if stats["ram_total_gb"] else 0.0
    faiss_size_gb = stats["faiss_size_mb"] / 1024.0

    RAM_USAGE_THRESHOLD = 0.85
    FAISS_SIZE_THRESHOLD_GB = 8.0

    return {
        "ram_usage_pct": ram_usage_pct,
        "faiss_size_gb": faiss_size_gb,
        "ram_safe": ram_usage_pct < RAM_USAGE_THRESHOLD,
        "faiss_safe": faiss_size_gb < FAISS_SIZE_THRESHOLD_GB,
        "overall_safe": (ram_usage_pct < RAM_USAGE_THRESHOLD) and (faiss_size_gb < FAISS_SIZE_THRESHOLD_GB),
    }


def get_theoretical_limits() -> Dict[str, Any]:
    """Limites aproximados (did√°ticos) para orientar o usu√°rio."""
    return {
        "max_vectors_estimate": 1_000_000,
        "max_docs_estimate": 20_000,
        "max_faiss_size_gb": 10.0,
        "context_window_tokens_estimate": 2048,
    }


# =============================================================================
# PDF -> texto -> chunks
# =============================================================================

def process_pdf(uploaded_file) -> List[Any]:
    """Extrai texto de um PDF e divide em trechos (chunks) com sobreposi√ß√£o."""
    tmp_file_path = None
    try:
        from langchain_community.document_loaders import PyPDFLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        loader = PyPDFLoader(tmp_file_path)
        pages = loader.load()

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )

        return splitter.split_documents(pages)

    except Exception as e:
        logger.error(f"Erro ao processar PDF: {e}")
        raise
    finally:
        if tmp_file_path and os.path.exists(tmp_file_path):
            try:
                os.unlink(tmp_file_path)
            except Exception as e:
                logger.warning(f"Erro ao deletar tempor√°rio: {e}")


def process_pdf_safely(uploaded_file, max_chunks_per_file: int = 200) -> Tuple[List[Any], List[str]]:
    """Processa PDF com seguran√ßa limitando a quantidade de chunks."""
    warnings: List[str] = []
    try:
        loaded_docs = process_pdf(uploaded_file)
        if len(loaded_docs) > max_chunks_per_file:
            warnings.append(f"Arquivo grande: cortado para {max_chunks_per_file} trechos.")
            return loaded_docs[:max_chunks_per_file], warnings
        return loaded_docs, warnings
    except Exception as e:
        warnings.append(f"Erro ao processar PDF: {str(e)}")
        return [], warnings


# =============================================================================
# Pipeline RAG (LangChain + HuggingFace + FAISS)
# =============================================================================

def format_response(response_data: Dict[str, Any]) -> Tuple[str, List[Any]]:
    """Normaliza a resposta do pipeline RAG para exibi√ß√£o (texto + documentos fonte)."""
    if isinstance(response_data, dict):
        answer = response_data.get("result", "Resposta n√£o encontrada.")
        source_docs = response_data.get("source_documents", [])
    else:
        answer = str(response_data)
        source_docs = []

    answer = answer.strip()
    if not answer:
        answer = "Desculpe, n√£o consegui gerar uma resposta adequada para sua pergunta."
    return answer, source_docs


@st.cache_resource(show_spinner=False)
def initialize_rag_pipeline(
    docs_texts: List[str],
    retriever_k: int,
    temperature: float,
    max_new_tokens: int,
):
    """
    Inicializa o pipeline RAG (com cache).

    Returns:
        (qa_chain, vectorstore, retriever, embeddings)
    """
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain.chains import RetrievalQA
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline as hf_pipeline
        from langchain_community.llms import HuggingFacePipeline

        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )

        vectorstore = FAISS.from_texts(docs_texts, embeddings)

        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": int(retriever_k)},
        )

        model_name = "google/flan-t5-base"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        gen_pipeline = hf_pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=int(max_new_tokens),
            temperature=float(temperature),
            do_sample=True,
            repetition_penalty=1.1,
        )

        llm = HuggingFacePipeline(pipeline=gen_pipeline)

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            verbose=False,
        )

        return qa_chain, vectorstore, retriever, embeddings

    except Exception as e:
        logger.error(f"Erro ao inicializar pipeline: {e}")
        return None, None, None, None


def ensure_pipeline_ready() -> None:
    """Garante que o pipeline esteja inicializado no estado da sess√£o."""
    if st.session_state.qa_chain is not None:
        return

    custom_docs = load_custom_docs()
    docs = custom_docs if custom_docs else get_default_docs()

    with st.spinner("Inicializando o modelo e a base de conhecimento‚Ä¶"):
        qa_chain, vectorstore, retriever, embeddings = initialize_rag_pipeline(
            docs_texts=docs,
            retriever_k=st.session_state.retriever_k,
            temperature=st.session_state.temperature,
            max_new_tokens=st.session_state.max_new_tokens,
        )

    if qa_chain is None:
        st.error("N√£o foi poss√≠vel inicializar o pipeline. Verifique depend√™ncias e tente novamente.")
        st.stop()

    st.session_state.qa_chain = qa_chain
    st.session_state.vectorstore = vectorstore
    st.session_state.retriever = retriever
    st.session_state.embeddings = embeddings
    st.session_state.docs = docs


# =============================================================================
# Sidebar (navega√ß√£o, configura√ß√µes, status)
# =============================================================================

def render_sidebar() -> None:
    """Barra lateral com navega√ß√£o, configura√ß√µes e status do sistema."""
    with st.sidebar:
        st.markdown(f"### {APP_SHORT}")
        st.caption("Navega√ß√£o e configura√ß√µes")

        st.session_state.page = st.radio(
            "Ir para",
            options=["Chat", "Documentos", "Gloss√°rio & Ajuda"],
            index=["Chat", "Documentos", "Gloss√°rio & Ajuda"].index(st.session_state.page),
            label_visibility="collapsed",
        )

        st.markdown("---")

        with st.expander("Configura√ß√µes do pipeline", expanded=True):
            st.caption("Ajustes que afetam busca e gera√ß√£o. Veja o significado no Gloss√°rio.")

            new_k = st.slider("Top-k (k): trechos recuperados", 1, 8, int(st.session_state.retriever_k))
            new_temp = st.slider("Temperatura (criatividade)", 0.0, 1.2, float(st.session_state.temperature), 0.05)
            new_tokens = st.slider("Tamanho m√°ximo da resposta (tokens)", 64, 1024, int(st.session_state.max_new_tokens), 32)

            cols = st.columns(2)
            apply_clicked = cols[0].button("Aplicar", use_container_width=True)
            reset_clicked = cols[1].button("Padr√£o", use_container_width=True)

            if reset_clicked:
                st.session_state.retriever_k = 3
                st.session_state.temperature = 0.7
                st.session_state.max_new_tokens = 512
                st.cache_resource.clear()
                st.session_state.qa_chain = None
                st.session_state.toast = "Configura√ß√µes restauradas para o padr√£o."
                st.rerun()

            if apply_clicked:
                st.session_state.retriever_k = int(new_k)
                st.session_state.temperature = float(new_temp)
                st.session_state.max_new_tokens = int(new_tokens)

                st.cache_resource.clear()
                st.session_state.qa_chain = None
                st.session_state.toast = "Configura√ß√µes aplicadas. O pipeline ser√° recarregado."
                st.rerun()

        st.markdown("---")

        stats = get_system_stats()
        safety = check_system_safety()

        status_color = "state-ok" if safety["overall_safe"] else "state-warn"
        st.markdown(f"**Status do sistema:** <span class='{status_color}'>‚óè</span>", unsafe_allow_html=True)
        st.caption("Recursos locais (para evitar travamentos).")

        st.write(f"RAM: {stats['ram_used_gb']:.1f} / {stats['ram_total_gb']:.1f} GB")
        st.write(f"Documentos indexados: {len(st.session_state.docs):,}")
        st.write(f"Vetores (FAISS): {int(stats['total_vectors']):,}")

        if not safety["overall_safe"]:
            st.warning("Uso alto de recursos. Considere reduzir PDFs ou quantidade de trechos.")


# =============================================================================
# P√°ginas
# =============================================================================

def page_chat() -> None:
    """P√°gina de chat (pergunta -> resposta + evid√™ncias)."""
    render_header()

    if st.session_state.toast:
        st.info(st.session_state.toast)
        st.session_state.toast = None

    col_left, col_right = st.columns([2.2, 1])

    with col_left:
        card(
            "Pergunte ao modelo",
            """
            Use perguntas curtas e diretas. Para respostas mais ‚Äúancoradas‚Äù, suba PDFs na aba **Documentos**.
            """,
            pills=["RAG", "LLM local", "Evid√™ncias"],
        )

        query = st.text_area(
            "Pergunta",
            placeholder="Ex.: O que √© RAG? Como embeddings ajudam na busca? O que √© Deep Learning?",
            height=110,
            key="chat_input",
            label_visibility="collapsed",
        )

        st.markdown("**Sugest√µes (clique para preencher):**")
        btn_cols = st.columns(3)

        examples = [
            ("O que √© RAG?", "Explique o que √© RAG e por que ele ajuda a reduzir alucina√ß√µes."),
            ("Embeddings", "O que s√£o representa√ß√µes vetoriais (embeddings) e para que servem?"),
            ("FAISS", "O que √© FAISS e como ele ajuda a encontrar documentos similares?"),
        ]

        for i, (label, text) in enumerate(examples):
            if btn_cols[i].button(label, use_container_width=True):
                st.session_state["chat_input"] = text
                st.rerun()

        ask = st.button("Buscar resposta", type="primary", use_container_width=True)

        if ask:
            if not query.strip():
                st.warning("Digite uma pergunta antes de buscar.")
                return

            safety = check_system_safety()
            if not safety["overall_safe"]:
                st.error("Recursos do sistema est√£o altos. Reduza documentos/trechos e tente novamente.")
                return

            with st.spinner("Processando‚Ä¶ (buscando trechos relevantes e gerando resposta)"):
                try:
                    start = time.time()
                    response = st.session_state.qa_chain.invoke({"query": query})
                    elapsed = time.time() - start

                    answer, source_docs = format_response(response)

                    st.session_state.query_history.append(
                        {
                            "query": query,
                            "answer": answer,
                            "time": elapsed,
                            "timestamp": time.strftime("%H:%M:%S"),
                        }
                    )

                    card(
                        "Resposta",
                        f"""
                        <b>Texto gerado:</b><br/>
                        {answer}
                        <br/><br/>
                        <span class="miguel-pill">Tempo: {elapsed:.2f}s</span>
                        """,
                        pills=["Resposta", "Tempo", "Clareza"],
                    )

                    if source_docs:
                        st.subheader("Evid√™ncias utilizadas (trechos recuperados)")
                        st.caption("Mostramos os trechos que mais contribu√≠ram para a resposta.")

                        for idx, doc in enumerate(source_docs[: min(3, len(source_docs))], 1):
                            with st.expander(f"Trecho {idx}", expanded=(idx == 1)):
                                st.write(doc.page_content)

                except Exception as e:
                    logger.error(f"Erro ao responder: {e}")
                    st.error("Ocorreu um erro ao gerar a resposta. Tente novamente ou reduza documentos.")

    with col_right:
        history = st.session_state.query_history
        total = len(history)

        # Evita formata√ß√£o de None como float
        if total:
            avg = sum(x["time"] for x in history) / total
            last = history[-1]["time"]
            avg_txt = f"{avg:.2f}s"
            last_txt = f"{last:.2f}s"
        else:
            avg_txt = "-"
            last_txt = "-"

        card(
            "M√©tricas da sess√£o",
            f"""
            - Total de perguntas: <b>{total}</b><br/>
            - Tempo m√©dio: <b>{avg_txt}</b><br/>
            - √öltima pergunta: <b>{last_txt}</b>
            """,
            pills=["Observa√ß√£o", "Transpar√™ncia"],
        )

        card(
            "Base de conhecimento",
            f"""
            - Documentos indexados: <b>{len(st.session_state.docs)}</b><br/>
            - Top-k atual: <b>{st.session_state.retriever_k}</b><br/><br/>
            Dica: envie PDFs na aba <b>Documentos</b> para enriquecer a base.
            """,
            pills=["Docs", "Top-k"],
        )

    if st.session_state.query_history:
        st.markdown("---")
        st.subheader("Hist√≥rico recente")
        for item in reversed(st.session_state.query_history[-5:]):
            with st.expander(f"{item['timestamp']} ‚Äî {item['query'][:60]}"):
                st.write(f"**Pergunta:** {item['query']}")
                st.write(f"**Resposta:** {item['answer']}")
                st.write(f"**Tempo:** {item['time']:.2f}s")


def page_documents() -> None:
    """P√°gina de upload/processamento de PDFs e atualiza√ß√£o do pipeline."""
    render_header()

    card(
        "Gerenciar base de conhecimento",
        """
        Envie PDFs para ampliar o conhecimento do sistema. O texto ser√° dividido em trechos (chunking),
        transformado em representa√ß√µes vetoriais (embeddings) e indexado no FAISS para busca por similaridade.
        """,
        pills=["PDF", "Chunking", "FAISS"],
    )

    uploaded_files = st.file_uploader(
        "Enviar PDFs",
        type=["pdf"],
        accept_multiple_files=True,
        help="Dica: comece com 1 PDF pequeno para observar o comportamento do RAG.",
    )

    if uploaded_files:
        st.caption(f"{len(uploaded_files)} arquivo(s) selecionado(s).")
        for f in uploaded_files:
            st.write(f"- {f.name} ({f.size / 1024:.1f} KB)")

        process = st.button("Processar e adicionar √† base", type="primary", use_container_width=True)

        if process:
            safety = check_system_safety()
            if not safety["overall_safe"]:
                st.error("Recursos do sistema est√£o altos. Envie PDFs menores ou feche outros apps.")
                return

            progress = st.progress(0)
            status = st.empty()

            new_texts: List[str] = []
            warnings_all: List[str] = []

            for i, f in enumerate(uploaded_files, start=1):
                status.text(f"Processando: {f.name}")
                progress.progress(int((i - 0.25) / len(uploaded_files) * 100) / 100)

                docs, warns = process_pdf_safely(f, max_chunks_per_file=200)
                warnings_all.extend([f"{f.name}: {w}" for w in warns])

                for d in docs:
                    new_texts.append(d.page_content)

                safety_now = check_system_safety()
                if not safety_now["ram_safe"]:
                    warnings_all.append("Uso de RAM alto: interrompemos o processamento para evitar travamento.")
                    break

                progress.progress(int(i / len(uploaded_files) * 100) / 100)

            if warnings_all:
                with st.expander("Avisos do processamento"):
                    for w in warnings_all:
                        st.warning(w)

            if not new_texts:
                st.warning("Nenhum texto foi extra√≠do. Tente outro PDF.")
                return

            status.text("Atualizando base e reinicializando pipeline‚Ä¶")
            updated_docs = (st.session_state.docs or []) + new_texts

            if save_custom_docs(updated_docs):
                st.cache_resource.clear()
                st.session_state.qa_chain = None
                st.session_state.docs = updated_docs
                ensure_pipeline_ready()
                status.text("Conclu√≠do.")
                st.success(f"{len(new_texts)} trechos adicionados √† base.")
            else:
                st.error("N√£o foi poss√≠vel salvar os documentos. Verifique permiss√µes de escrita.")
                return

    st.markdown("---")

    stats = get_system_stats()
    limits = get_theoretical_limits()

    cols = st.columns(3)
    cols[0].metric("Documentos (strings)", int(stats["total_docs"]))
    cols[1].metric("Vetores (FAISS)", int(stats["total_vectors"]))
    cols[2].metric("FAISS (estimado)", f"{stats['faiss_size_mb']:.1f} MB")

    with st.expander("Ver amostra dos documentos indexados"):
        docs = st.session_state.docs or []
        for i, d in enumerate(docs[:8], 1):
            st.write(f"{i}. {d[:180]}{'‚Ä¶' if len(d) > 180 else ''}")
        if len(docs) > 8:
            st.caption(f"‚Ä¶ e mais {len(docs) - 8} documento(s).")

    with st.expander("Limites (refer√™ncias did√°ticas)"):
        st.write(f"- M√°x. vetores (estimativa): {limits['max_vectors_estimate']:,}")
        st.write(f"- M√°x. documentos (estimativa): {limits['max_docs_estimate']:,}")
        st.write(f"- M√°x. FAISS (estimativa): {limits['max_faiss_size_gb']:.1f} GB")
        st.write(f"- Janela de contexto (estimativa): {limits['context_window_tokens_estimate']} tokens")

    st.markdown("---")

    st.subheader("Gerenciamento avan√ßado")
    col_a, col_b = st.columns(2)

    if col_a.button("Recarregar pipeline", use_container_width=True):
        st.cache_resource.clear()
        st.session_state.qa_chain = None
        st.session_state.toast = "Pipeline recarregado."
        st.rerun()

    with col_b:
        confirm = st.checkbox("Confirmo que desejo voltar para a base padr√£o (isso remove meus PDFs)")
        if st.button("Resetar base", use_container_width=True, disabled=not confirm):
            try:
                if os.path.exists(CUSTOM_DOCS_PATH):
                    os.remove(CUSTOM_DOCS_PATH)
                st.cache_resource.clear()
                st.session_state.qa_chain = None
                st.session_state.docs = get_default_docs()
                st.session_state.toast = "Base resetada para o padr√£o."
                st.rerun()
            except Exception as e:
                logger.error(f"Erro ao resetar base: {e}")
                st.error("N√£o foi poss√≠vel resetar. Verifique permiss√µes de arquivo.")


def page_glossary_help() -> None:
    """P√°gina de ajuda: gloss√°rio + heur√≠sticas de Nielsen + Gestalt."""
    render_header()

    card(
        "Gloss√°rio (termos t√©cnicos do aplicativo)",
        """
        Selecione um termo e veja <b>o que √©</b>, <b>onde aparece no app</b> e <b>por que importa</b>.
        A ideia √© permitir explora√ß√£o sem depender de jarg√µes.
        """,
        pills=["Did√°tico", "Autoexplicativo"],
    )

    term = st.selectbox("Escolha um termo", list(GLOSSARY.keys()))
    info = GLOSSARY[term]

    card(
        term,
        f"""
        <b>O que √©:</b> {info["o_que_e"]}<br/><br/>
        <b>Onde aparece no aplicativo:</b> {info["onde_aparece_no_app"]}<br/><br/>
        <b>Por que √© importante:</b> {info["por_que_importa"]}
        """,
        pills=["Defini√ß√£o", "Uso no app", "Import√¢ncia"],
    )

    st.markdown("---")

    card(
        "Heur√≠sticas de Nielsen (como melhoramos a usabilidade)",
        """
        <b>Visibilidade do status:</b> spinners, progresso e m√©tricas (tempo, docs, vetores).<br/>
        <b>Correspond√™ncia com o mundo real:</b> termos simples (‚Äútrechos‚Äù, ‚Äúevid√™ncias‚Äù, ‚Äúpergunta‚Äù).<br/>
        <b>Controle e liberdade:</b> reset com confirma√ß√£o e recarregar pipeline.<br/>
        <b>Consist√™ncia e padr√µes:</b> navega√ß√£o fixa e r√≥tulos uniformes.<br/>
        <b>Preven√ß√£o de erros:</b> limites de chunks e checagem de recursos.<br/>
        <b>Reconhecimento em vez de memoriza√ß√£o:</b> exemplos clic√°veis e hist√≥rico.<br/>
        <b>Design minimalista:</b> foco no essencial e bom contraste.<br/>
        <b>Ajuda e documenta√ß√£o:</b> gloss√°rio e textos de apoio.
        """,
        pills=["Nielsen", "UX", "Did√°tica"],
    )

    card(
        "Princ√≠pios da Gestalt (o que voc√™ v√™ na interface)",
        """
        <b>Proximidade:</b> itens relacionados ficam juntos (ex.: configura√ß√µes do pipeline).<br/>
        <b>Similaridade:</b> cards e blocos com o mesmo estilo, facilitando leitura r√°pida.<br/>
        <b>Regi√£o comum:</b> bordas e fundos agrupam conceitos (ex.: ‚ÄúResposta‚Äù e ‚ÄúEvid√™ncias‚Äù).<br/>
        <b>Figura-fundo:</b> contraste alto melhora legibilidade.<br/>
        <b>Continuidade:</b> fluxo de leitura vertical (Pergunta ‚Üí Resposta ‚Üí Evid√™ncias ‚Üí Hist√≥rico).<br/>
        <b>Fechamento:</b> expanders exibem detalhes sem poluir a tela.
        """,
        pills=["Gestalt", "Layout", "Percep√ß√£o"],
    )


# =============================================================================
# Main
# =============================================================================

def main() -> None:
    """Ponto de entrada do aplicativo."""
    inject_minimal_css()
    init_session_state()
    render_sidebar()

    ensure_pipeline_ready()

    if st.session_state.page == "Chat":
        page_chat()
    elif st.session_state.page == "Documentos":
        page_documents()
    else:
        page_glossary_help()

    st.markdown("---")
    st.caption(
        f"{APP_SHORT} ‚Äî aplica√ß√£o did√°tica, local e gratuita | "
        "Tecnologias: LangChain, HuggingFace, FAISS, Streamlit"
    )


if __name__ == "__main__":
    main()
