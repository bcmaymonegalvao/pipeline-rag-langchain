import streamlit as st
import time
import os
from typing import List, Dict, Any
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="ğŸš€ Pipeline RAG LangChain",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        background: linear-gradient(90deg, #6c5ce7, #a29bfe);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 1rem;
    }
    
    .sub-header {
        text-align: center;
        color: #636e72;
        font-size: 1.2rem;
        margin-bottom: 2rem;
    }
    
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
    }
    
    .doc-card {
        background: #f8f9fa;
        border-left: 4px solid #6c5ce7;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0 10px 10px 0;
    }
    
    .response-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 15px 0 rgba(31, 38, 135, 0.37);
    }
    
    .stAlert {
        border-radius: 10px;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource(show_spinner=False)
def initialize_rag_pipeline():
    """Inicializa o pipeline RAG com cache para otimizar performance"""
    try:
        with st.spinner("ğŸ”§ Inicializando pipeline RAG..."):
            # ImportaÃ§Ãµes necessÃ¡rias - ordem correta
            import torch
            from sentence_transformers import SentenceTransformer
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from langchain_community.vectorstores import FAISS
            from langchain.chains import RetrievalQA
            
            # Importar transformers por Ãºltimo para evitar conflitos
            from transformers import (
                AutoTokenizer, 
                AutoModelForSeq2SeqLM, 
                pipeline as hf_pipeline  # Renomear para evitar conflito
            )
            from langchain_community.llms import HuggingFacePipeline
            
            # Documentos de exemplo (podem ser expandidos)
            docs = [
                "O churn Ã© o cancelamento ou abandono de clientes em um serviÃ§o ou produto. Ã‰ uma mÃ©trica crucial para avaliar a retenÃ§Ã£o e satisfaÃ§Ã£o do cliente.",
                "NPS, ou Net Promoter Score, mede a lealdade dos clientes atravÃ©s da pergunta: 'VocÃª recomendaria nossa empresa a um amigo?' Varia de -100 a +100.",
                "LangChain Ã© uma biblioteca para construir aplicaÃ§Ãµes que usam modelos de linguagem large (LLMs) integrados a outras fontes de dados e ferramentas.",
                "RAG, Retrieval-Augmented Generation, conecta modelos de linguagem a bases de conhecimento atravÃ©s de embeddings e mecanismos de busca para melhorar respostas.",
                "Embeddings representam texto em vetores numÃ©ricos que capturam significado semÃ¢ntico, possibilitando busca eficiente por similaridade.",
                "O pipeline bÃ¡sico de RAG inclui: criaÃ§Ã£o de embeddings, uso do retriever para buscar documentos relevantes e geraÃ§Ã£o da resposta pelo LLM.",
                "Machine Learning Ã© um subcampo da inteligÃªncia artificial que permite que sistemas aprendam e melhorem automaticamente a partir da experiÃªncia.",
                "Deep Learning utiliza redes neurais artificiais com mÃºltiplas camadas para modelar e entender dados complexos como imagens, texto e Ã¡udio.",
                "Natural Language Processing (NLP) Ã© a Ã¡rea da IA focada na interaÃ§Ã£o entre computadores e linguagem humana natural.",
                "Business Intelligence (BI) envolve estratÃ©gias e tecnologias para anÃ¡lise de informaÃ§Ãµes de negÃ³cios e suporte Ã  tomada de decisÃ£o."
            ]
            
            # Configurar embeddings
            st.info("ğŸ“Š Carregando modelo de embeddings MiniLM...")
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # Criar Ã­ndice FAISS
            st.info("ğŸ—ƒï¸ Construindo Ã­ndice vetorial FAISS...")
            vectorstore = FAISS.from_texts(docs, embeddings)
            
            # Configurar retriever
            retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}  # Aumentamos para 3 para mais contexto
            )
            
            # Configurar LLM local
            st.info("ğŸ¤– Carregando modelo FLAN-T5...")
            model_name = "google/flan-t5-base"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            
            gen_pipeline = hf_pipeline(
                "text2text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                repetition_penalty=1.1
            )
            
            llm = HuggingFacePipeline(pipeline=gen_pipeline)
            
            # Criar chain RAG
            st.info("ğŸ”— Configurando chain RAG...")
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,  # Para mostrar fontes
                verbose=False
            )
            
            st.success("âœ… Pipeline RAG inicializado com sucesso!")
            
            return qa_chain, vectorstore, docs, retriever
            
    except Exception as e:
        st.error(f"âŒ Erro ao inicializar pipeline: {str(e)}")
        logger.error(f"Erro na inicializaÃ§Ã£o: {e}")
        return None, None, None, None

def format_response(response_data: Dict[str, Any]) -> tuple:
    """Formata a resposta do RAG para exibiÃ§Ã£o"""
    if isinstance(response_data, dict):
        answer = response_data.get("result", "Resposta nÃ£o encontrada")
        source_docs = response_data.get("source_documents", [])
    else:
        answer = str(response_data)
        source_docs = []
    
    # Limpar resposta
    answer = answer.strip()
    if not answer or answer.lower() in ["", "none", "null"]:
        answer = "Desculpe, nÃ£o consegui gerar uma resposta adequada para sua pergunta."
    
    return answer, source_docs

def main():
    # Header principal
    st.markdown('<div class="main-header">ğŸš€ Pipeline RAG LangChain</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Retrieval-Augmented Generation com HuggingFace & FAISS</div>', unsafe_allow_html=True)
    
    # Sidebar com informaÃ§Ãµes
    with st.sidebar:
        st.header("â„¹ï¸ Sobre o Sistema")
        st.markdown("""
        **ğŸ¯ Tecnologias:**
        - ğŸ§  **LLM**: FLAN-T5 (Google)
        - ğŸ” **Embeddings**: MiniLM (HuggingFace) 
        - ğŸ“Š **Vector Store**: FAISS
        - ğŸ”— **Framework**: LangChain
        
        **âš¡ CaracterÃ­sticas:**
        - âœ… 100% Gratuito (sem API keys)
        - ğŸ–¥ï¸ Roda localmente
        - ğŸš€ Interface moderna
        - ğŸ“š Base de conhecimento expandÃ­vel
        """)
        
        st.header("ğŸ“‹ Como usar")
        st.markdown("""
        1. **Digite sua pergunta** no campo abaixo
        2. **Clique em "Buscar Resposta"**
        3. **Veja a resposta** e documentos fonte
        4. **Explore diferentes temas**: IA, ML, NLP, BI
        """)
        
        # EstatÃ­sticas do sistema
        st.header("ğŸ“Š Sistema")
        if 'qa_chain' in st.session_state:
            st.success("ğŸŸ¢ Pipeline Ativo")
            st.info(f"ğŸ“„ Documentos: {len(st.session_state.docs)}")
            st.info("ğŸ” Retriever: Top-3")
        else:
            st.warning("ğŸŸ¡ Inicializando...")

    # Inicializar pipeline (apenas uma vez)
    if 'qa_chain' not in st.session_state:
        qa_chain, vectorstore, docs, retriever = initialize_rag_pipeline()
        if qa_chain:
            st.session_state.qa_chain = qa_chain
            st.session_state.vectorstore = vectorstore
            st.session_state.docs = docs
            st.session_state.retriever = retriever
            st.session_state.query_history = []
        else:
            st.stop()
    
    # Interface principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’­ FaÃ§a sua pergunta")
        
        # Campo de entrada da pergunta
        query = st.text_area(
            "Digite sua pergunta aqui:",
            placeholder="Ex: O que Ã© RAG? Como funciona machine learning? O que significa churn?",
            height=100,
            help="Digite qualquer pergunta sobre IA, ML, NLP, BI ou conceitos relacionados"
        )
        
        # BotÃµes de exemplo
        st.markdown("**ğŸ’¡ Perguntas de exemplo:**")
        col_btn1, col_btn2, col_btn3 = st.columns(3)
        
        with col_btn1:
            if st.button("ğŸ”„ O que Ã© churn?"):
                query = "O que significa churn em anÃ¡lise de clientes?"
        
        with col_btn2:
            if st.button("ğŸ¤– Como funciona RAG?"):
                query = "Como funciona o pipeline de RAG?"
        
        with col_btn3:
            if st.button("ğŸ“š O que Ã© LangChain?"):
                query = "Explique o que Ã© LangChain e para que serve."
        
        # BotÃ£o de busca
        search_button = st.button("ğŸ” Buscar Resposta", type="primary", use_container_width=True)
        
        # Processar pergunta
        if search_button and query.strip():
            with st.spinner("ğŸ¤” Processando sua pergunta..."):
                try:
                    # Registrar tempo de resposta
                    start_time = time.time()
                    
                    # Executar RAG
                    response = st.session_state.qa_chain.invoke({"query": query})
                    
                    # Calcular tempo
                    response_time = time.time() - start_time
                    
                    # Formatar resposta
                    answer, source_docs = format_response(response)
                    
                    # Adicionar ao histÃ³rico
                    st.session_state.query_history.append({
                        "query": query,
                        "answer": answer,
                        "time": response_time,
                        "timestamp": time.strftime("%H:%M:%S")
                    })
                    
                    # Exibir resposta
                    st.markdown('<div class="response-card">', unsafe_allow_html=True)
                    st.markdown(f"**ğŸ¤– Resposta:**\n\n{answer}")
                    st.markdown(f"â±ï¸ *Tempo de resposta: {response_time:.2f}s*")
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Mostrar documentos fonte
                    if source_docs:
                        st.markdown("**ğŸ“„ Documentos utilizados:**")
                        for i, doc in enumerate(source_docs[:2], 1):
                            with st.expander(f"ğŸ“‹ Fonte {i}"):
                                st.markdown(f'<div class="doc-card">{doc.page_content}</div>', 
                                          unsafe_allow_html=True)
                    
                except Exception as e:
                    st.error(f"âŒ Erro ao processar pergunta: {str(e)}")
                    logger.error(f"Erro no processamento: {e}")
        
        elif search_button:
            st.warning("âš ï¸ Por favor, digite uma pergunta antes de buscar!")
    
    with col2:
        st.header("ğŸ“ˆ EstatÃ­sticas")
        
        # MÃ©tricas
        if 'query_history' in st.session_state and st.session_state.query_history:
            total_queries = len(st.session_state.query_history)
            avg_time = sum(q["time"] for q in st.session_state.query_history) / total_queries
            last_query_time = st.session_state.query_history[-1]["time"]
            
            st.metric("ğŸ”¢ Total de Perguntas", total_queries)
            st.metric("â±ï¸ Tempo MÃ©dio", f"{avg_time:.2f}s")
            st.metric("ğŸ• Ãšltima Consulta", f"{last_query_time:.2f}s")
        else:
            st.metric("ğŸ”¢ Total de Perguntas", "0")
            st.metric("â±ï¸ Tempo MÃ©dio", "-")
            st.metric("ğŸ• Ãšltima Consulta", "-")
        
        # Base de conhecimento
        st.header("ğŸ“š Base de Conhecimento")
        st.info(f"ğŸ“„ {len(st.session_state.docs)} documentos indexados")
        
        with st.expander("ğŸ‘ï¸ Ver documentos"):
            for i, doc in enumerate(st.session_state.docs, 1):
                st.markdown(f"**{i}.** {doc[:100]}...")
    
    # HistÃ³rico de perguntas
    if 'query_history' in st.session_state and st.session_state.query_history:
        st.header("ğŸ“‹ HistÃ³rico de Perguntas")
        
        # Mostrar apenas as Ãºltimas 5 perguntas
        recent_history = st.session_state.query_history[-5:]
        
        for item in reversed(recent_history):
            with st.expander(f"ğŸ• {item['timestamp']} - {item['query'][:50]}..."):
                st.markdown(f"**Pergunta:** {item['query']}")
                st.markdown(f"**Resposta:** {item['answer']}")
                st.markdown(f"**Tempo:** {item['time']:.2f}s")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #636e72;'>
        ğŸš€ Desenvolvido com Streamlit | ğŸ¤– Powered by HuggingFace & LangChain
        <br>
        ğŸ’¡ Sistema RAG 100% gratuito e local
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()